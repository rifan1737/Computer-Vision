{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rifan1737/Computer-Vision/blob/main/05_02_Fire_Rifan_Ahmad_Rifandi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avr_BuLrV23M"
      },
      "source": [
        "# Assignment Ch. 5 - Transfer Learning [Case #2]\n",
        "Startup Campus, Indonesia - `Artificial Intelligence Track`\n",
        "* Dataset: MNIST Handwritten Digits (10 classes)\n",
        "* Libraries: PyTorch, Torchvision, Scikit-learn\n",
        "* Objective: Transfer Learning using CNN-based Pre-trained Models\n",
        "\n",
        "`PREREQUISITE` All modules (with their suitable versions) are installed properly.\n",
        "<br>`TASK` Complete the notebook cell's code marked with <b>#TODO</b> comment.\n",
        "<br>`TARGET PORTFOLIO` Students are able to:\n",
        "* implement transfer learning technique using various PyTorch pre-trained models, and\n",
        "* examine the effect of freezing some parts of the layer.\n",
        "\n",
        "<br>`WARNING` Do **NOT CHANGE** any codes within the User-defined Functions (UDFs) section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRm6PPoYV23R"
      },
      "source": [
        "### Case Study Description\n",
        "A new robotic facility located in East Kalimantan, near the Titik Nol Ibu Kota Negara (IKN) Indonesia, asks you to create a Computer Vision model for their new droid (robot) products. The company requests you to **teach the robot how to read a sequence of numbers**. You suddenly realize that the first stage is to let the robot correctly identify each individual digit (0-9). However, since the prototype announcement date was hastened, your deadline is very tight: you only have **less than 1 week** to complete the job. As a professional AI developer, you keep calm and know that you can exploit the **Transfer Learning** method to solve this problem efficiently.\n",
        "\n",
        "As a basic dataset in most of Computer Vision tasks, **Modified National Institute of Standards and Technology (MNIST) database** contains 10 handwritten digits. All of them are in the grayscale (1-channel). Torchvision, a sub-library of PyTorch, has dozens of pre-trained models that you can easily choose from. All of these models were originally trained on the ImageNet dataset [(ref1)](https://www.image-net.org/download.php), which contains millions of RGB (3-channel) images and 1,000 classes. For simplicity, let choose **Resnet18** [(ref2)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), **DenseNet121** [(ref3)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf), and **Vision Transformer (ViT)** [(ref4)](https://arxiv.org/pdf/2010.11929.pdf) as baseline, state-of-the-art models to test the **image classification** performance. Your complete tasks are as follows.\n",
        "\n",
        "1. Pick **DenseNet** as your first model to experiment with, then **change the number of neurons in the first and last layers** (since the ImageNet has 1,000 classes, while MNIST only has 10 classes; both are also come with different image size and channel).\n",
        "2. Define **hyperparameters** and train the model (all **layers are trainable**).\n",
        "3. Plot the model performance, for both **training** and **validation** results.\n",
        "4. Now try to **freeze (layers are non-trainable) some parts** of layers: (1) \"denseblock1\", (2) \"denseblock1\" and \"denseblock2\". These will be two separate models.\n",
        "5. **Retrain** each model, plot its performance, and examine the difference.\n",
        "6. BONUS: Can you **replicate** all of the steps above with different models, i.e., **ResNet** and **ViT**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Hk1Q4RV23U"
      },
      "source": [
        "[KLIK UNTUK TERJEMAHKAN TEKS](https://translate.google.com/?hl=id&ie=UTF-8&sl=en&tl=id&text=A%20new%20robotic%20facility%20located%20in%20East%20Kalimantan%2C%20near%20the%20Titik%20Nol%20Ibu%20Kota%20Negara%20(IKN)%20Indonesia%2C%20asks%20you%20to%20create%20a%20Computer%20Vision%20model%20for%20their%20new%20droid%20(robot)%20products.%20The%20company%20requests%20you%20to%20teach%20the%20robot%20how%20to%20read%20a%20sequence%20of%20numbers.%20You%20suddenly%20realize%20that%20the%20first%20stage%20is%20to%20let%20the%20robot%20correctly%20identify%20each%20individual%20digit%20(0-9).%20However%2C%20since%20the%20prototype%20announcement%20date%20was%20hastened%2C%20your%20deadline%20is%20very%20tight%3A%20you%20only%20have%20less%20than%201%20week%20to%20complete%20the%20job.%20As%20a%20professional%20AI%20developer%2C%20you%20keep%20calm%20and%20know%20that%20you%20can%20exploit%20the%20Transfer%20Learning%20method%20to%20solve%20this%20problem%20efficiently.%0A%0AAs%20a%20basic%20dataset%20in%20most%20of%20Computer%20Vision%20tasks%2C%20Modified%20National%20Institute%20of%20Standards%20and%20Technology%20(MNIST)%20database%20contains%2010%20handwritten%20digits.%20All%20of%20them%20are%20in%20the%20grayscale%20(1-channel).%20Torchvision%2C%20a%20sub-library%20of%20PyTorch%2C%20has%20dozens%20of%20pre-trained%20models%20that%20you%20can%20easily%20choose%20from.%20All%20of%20these%20models%20were%20originally%20trained%20on%20the%20ImageNet%20dataset%20(ref1)%2C%20which%20contains%20millions%20of%20RGB%20(3-channel)%20images%20and%201%2C000%20classes.%20For%20simplicity%2C%20let%20choose%20Resnet18%20(ref2)%2C%20DenseNet121%20(ref3)%2C%20and%20Vision%20Transformer%20(ViT)%20(ref4)%20as%20baseline%2C%20state-of-the-art%20models%20to%20test%20the%20image%20classification%20performance.%20Your%20complete%20tasks%20are%20as%20follows.%0A%0A1.%20Pick%20DenseNet%20as%20your%20first%20model%20to%20experiment%20with%2C%20then%20change%20the%20number%20of%20neurons%20in%20the%20first%20and%20last%20layers%20(since%20the%20ImageNet%20has%201%2C000%20classes%2C%20while%20MNIST%20only%20has%2010%20classes%3B%20both%20are%20also%20come%20with%20different%20image%20size%20and%20channel).%0A%0A2.%20Define%20hyperparameters%20and%20train%20the%20model%20(all%20layers%20are%20trainable).%0A%0A3.%20Plot%20the%20model%20performance%2C%20for%20both%20training%20and%20validation%20results.%0A%0A4.%20Now%20try%20to%20freeze%20(layers%20are%20non-trainable)%20some%20parts%20of%20layers%3A%20(1)%20%22denseblock1%22%2C%20(2)%20%22denseblock1%22%20and%20%22denseblock2%22.%20These%20will%20be%20two%20separate%20models.%0A%0A5.%20Retrain%20each%20model%2C%20plot%20its%20performance%2C%20and%20examine%20the%20difference.%0A%0A6.%20BONUS%3A%20Can%20you%20replicate%20all%20of%20the%20steps%20above%20with%20different%20models%2C%20i.e.%2C%20ResNet%20and%20ViT%3F&op=translate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWlZH8_X1-Jc"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EauNKxXsSznF"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision, time\n",
        "from numpy.random import seed\n",
        "from tqdm.autonotebook import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from copy import deepcopy\n",
        "from warnings import filterwarnings as fw; fw(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JMpiYWmV23Z"
      },
      "outputs": [],
      "source": [
        "torch.__version__ == \"2.0.1+cu117\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYpJvCkeV23a"
      },
      "outputs": [],
      "source": [
        "torchvision.__version__ == \"0.15.2+cu117\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xNi2ZmbV23a"
      },
      "outputs": [],
      "source": [
        "# define seeding\n",
        "seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqOqMRIMV23b"
      },
      "source": [
        "### User-defined Functions (UDFs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwAXvOdqV23c"
      },
      "source": [
        "- To print total model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4Gf78cdV23c"
      },
      "outputs": [],
      "source": [
        "def check_params(model, *args, **kwargs) -> dict:\n",
        "    return {\n",
        "        \"total_trainable_params\" : sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "        \"total_nontrainable_params\" : sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us3iI6FrV23d"
      },
      "source": [
        "- To get the pair of train and validation dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Aw_XFl8oCOZ"
      },
      "outputs": [],
      "source": [
        "data_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((224, 224)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((torch.tensor(33.3184)/255,), (torch.tensor(78.5675)/255,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root=\".\", train=True, transform=data_transform, download=True).train_data.float()\n",
        "\n",
        "def get_dataloaders(train_batch_size : int, val_batch_size : int, max_rows : int = 1000, *args, **kwargs) -> tuple:\n",
        "    data_transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize((224, 224)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((torch.tensor(33.3184)/255,), (torch.tensor(78.5675)/255,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.MNIST(root=\".\", train=True, transform=data_transform)\n",
        "    train_idx = torch.randperm(len(train_dataset))[:int(max_rows*.75)]\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, \\\n",
        "                                               sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
        "\n",
        "    val_dataset = torchvision.datasets.MNIST(root=\".\", train=False, transform=data_transform)\n",
        "    val_idx = torch.randperm(len(val_dataset))[:int(max_rows*.25)]\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size, \\\n",
        "                                             sampler=torch.utils.data.SubsetRandomSampler(val_idx))\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xHpXQlkV23e"
      },
      "source": [
        "* To fit (training) the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m05rFpG5f5yn",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "def fit(\n",
        "    model : torchvision.models,\n",
        "    epoch : int,\n",
        "    train_loader : torch.utils.data.DataLoader,\n",
        "    val_loader : torch.utils.data.DataLoader,\n",
        "    *args, **kwargs\n",
        ") -> dict:\n",
        "\n",
        "    TRAIN_LOSS, TRAIN_ACC = [], []\n",
        "    train_batches = len(train_loader)\n",
        "\n",
        "    VAL_LOSS, VAL_ACC = [], []\n",
        "    val_batches = len(val_loader)\n",
        "\n",
        "    # loop for every epoch (training + evaluation)\n",
        "    start_ts = time.time()\n",
        "    for e in range(epoch):\n",
        "        train_losses = 0\n",
        "        train_accuracies = 0\n",
        "\n",
        "        # progress bar\n",
        "        progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=train_batches)\n",
        "\n",
        "        # ----------------- TRAINING  --------------------\n",
        "        # set model to training\n",
        "        model.train()\n",
        "\n",
        "        for i, data in progress:\n",
        "            X, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # training step for single batch\n",
        "            model.zero_grad()\n",
        "\n",
        "            # forward pass\n",
        "            outputs = model(X)\n",
        "            loss = loss_function(outputs, y)\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses += loss.item()\n",
        "\n",
        "            ps = torch.exp(outputs)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == y.view(*top_class.shape)\n",
        "            train_accuracies += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "            # updating progress bar\n",
        "            progress.set_description(\"Loss: {:.4f}\".format(train_losses/(i+1)))\n",
        "\n",
        "        TRAIN_ACC.append(train_accuracies/train_batches)\n",
        "        TRAIN_LOSS.append(train_losses/train_batches)\n",
        "\n",
        "        # releasing unceseccary memory in GPU\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # ----------------- VALIDATION  -----------------\n",
        "        val_losses = 0\n",
        "        val_accuracies = 0\n",
        "\n",
        "        # set model to evaluating (testing)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                X, y = data[0].to(device), data[1].to(device)\n",
        "                outputs = model(X) # this gives the prediction from the network\n",
        "                val_losses += loss_function(outputs, y).item()\n",
        "\n",
        "                ps = torch.exp(outputs)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == y.view(*top_class.shape)\n",
        "                val_accuracies += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "        print(\"Epoch {}/{} >> Training loss: {:.3f}, Validation loss: {:.3f}, Validation accuracy: {:.3f}\".format(\n",
        "            e+1, epoch, train_losses/train_batches, val_losses/val_batches, val_accuracies/val_batches*100)\n",
        "        )\n",
        "\n",
        "        VAL_ACC.append(val_accuracies/val_batches)\n",
        "        VAL_LOSS.append(val_losses/val_batches)\n",
        "\n",
        "    tr_time = time.time()-start_ts\n",
        "    print(\"Training time: {:.3f}s\".format(tr_time))\n",
        "\n",
        "    return {\n",
        "        \"model\" : model.name,\n",
        "        \"train_acc\" : TRAIN_ACC,\n",
        "        \"train_loss\" : TRAIN_LOSS,\n",
        "        \"val_acc\" : VAL_ACC,\n",
        "        \"val_loss\" : VAL_LOSS,\n",
        "        \"exc_time\" : tr_time\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALnyfhdrV23f"
      },
      "source": [
        "* To visualize the model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si1GvGKHV23f"
      },
      "outputs": [],
      "source": [
        "def plot_performance(dict_ : dict, *args, **kwargs) -> None:\n",
        "    my_figure = plt.figure(figsize=(12, 4))\n",
        "    # NOTE: figsize=(width/horizontally, height/vertically)\n",
        "\n",
        "    m = my_figure.add_subplot(121)\n",
        "    plt.plot(dict_[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(dict_[\"val_loss\"], label=\"Valid. Loss\")\n",
        "    plt.title(\"LOSS\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    n = my_figure.add_subplot(122)\n",
        "    plt.plot(dict_[\"train_acc\"], label=\"Train Accuracy\")\n",
        "    plt.plot(dict_[\"val_acc\"], label=\"Valid. Accuracy\")\n",
        "    plt.title(\"ACCURACY\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYOweBmFV23g"
      },
      "source": [
        "### Define the model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzkK82Swc4ca"
      },
      "outputs": [],
      "source": [
        "class VisionModel(torch.nn.Module):\n",
        "    def __init__(self, model_selection : str, *args, **kwargs) -> None:\n",
        "        super(VisionModel, self).__init__()\n",
        "        self.model_selection = self.name = model_selection\n",
        "        self.in_channels = 1\n",
        "\n",
        "        def create_conv2d(this_layer, *args, **kwargs) -> torch.nn.modules.conv.Conv2d:\n",
        "            return torch.nn.Conv2d(\n",
        "                in_channels=self.in_channels, out_channels=this_layer.out_channels,\n",
        "                kernel_size=this_layer.kernel_size, stride=this_layer.stride,\n",
        "                padding=this_layer.padding, bias=this_layer.bias\n",
        "            )\n",
        "\n",
        "        if not self.model_selection.lower() in [\"resnet\", \"densenet\", \"vit\"]:\n",
        "            raise ValueError(\"Please select the model: 'resnet', 'densenet', or 'vit'.\")\n",
        "\n",
        "        if self.model_selection == \"resnet\":\n",
        "            self.model = torchvision.models.resnet18(pretrained=True)\n",
        "            self.model.conv1 = create_conv2d(self.model.conv1) # change the input layer to take Grayscale image, instead of RGB\n",
        "            self.model.fc = torch.nn.Linear(self.model.fc.in_features, 10) # change the output layer to output 10 classes\n",
        "\n",
        "        elif self.model_selection == \"densenet\":\n",
        "            self.model = torchvision.models.densenet121(pretrained=True)\n",
        "            self.model.features.conv0 = ... # TODO: Change the DenseNet input layer stack by calling create_conv2d()\n",
        "            self.model.classifier = ... # TODO: Change the DenseNet output layer with 10 classes\n",
        "\n",
        "        elif self.model_selection == \"vit\":\n",
        "            self.model = torchvision.models.vit_b_16(pretrained=True)\n",
        "            self.model.conv_proj = ... # TODO: Change the ViT input layer stack by calling create_conv2d()\n",
        "            self.model.classifier = # TODO: Change the ViT output layer with 10 classes\n",
        "\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, data, *args, **kwargs) -> torchvision.models:\n",
        "        x = self.model(data)\n",
        "        return self.softmax(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5W0KXU72-PT"
      },
      "source": [
        "### Set device to CUDA\n",
        "On your Google Collab, click Runtime > Change Runtime Type > then select T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcXwCjR1Ylkv"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT-Q8MHzV23i"
      },
      "source": [
        "### Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhYwVrbBV23i"
      },
      "outputs": [],
      "source": [
        "EPOCH = 5\n",
        "BATCH_SIZE = ... # TODO: Define the batch size\n",
        "LEARNING_RATE = ... # TODO: Define the learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i72AP_NXV23j"
      },
      "source": [
        "### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEr4mIDdV23j"
      },
      "outputs": [],
      "source": [
        "# TODO: Pass the string \"resnet\" for ResNet18, \"densenet\" for DenseNet121, and \"vit\" for Vision Transformer\n",
        "model = VisionModel(...).to(device)\n",
        "check_params(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "IAEbBYGvV23j"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-n1bdNDV23k"
      },
      "source": [
        "### WILL BE USED LATER: Freeze some layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orYYwn0ZV23k"
      },
      "outputs": [],
      "source": [
        "model_freeze_block1 = deepcopy(model)\n",
        "for name, param in model_freeze_block1.named_parameters():\n",
        "    if param.requires_grad and \"denseblock1\" in name:\n",
        "        param.requires_grad = False\n",
        "check_params(model_freeze_block1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjiUG3w1V23k"
      },
      "outputs": [],
      "source": [
        "model_freeze_block12 = deepcopy(model)\n",
        "for name, param in model_freeze_block12.named_parameters():\n",
        "    if param.requires_grad and any([x in name for x in [\"denseblock1\", \"denseblock2\"]]):\n",
        "        param.requires_grad = False\n",
        "check_params(model_freeze_block12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8CslUMA1FZD"
      },
      "source": [
        "### Get train and validation dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn3YgFa_V23l"
      },
      "source": [
        "To speedup the training time, we will only use 1,000 (of 60,000) images from MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnEsZPvpV23l"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader = get_dataloaders(BATCH_SIZE, BATCH_SIZE)\n",
        "len(train_loader), len(val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpktUiY9V23m"
      },
      "source": [
        "### Set loss function and model optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APMQehx_V23m"
      },
      "outputs": [],
      "source": [
        "loss_function = ... # Define the loss function (for multi-classification)\n",
        "\n",
        "trainable_model_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.Adam(trainable_model_params, lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F6Qym3NV23p"
      },
      "source": [
        "### Start the model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPAy2FwgV23p"
      },
      "outputs": [],
      "source": [
        "# TODO: Specify variables for your model, number of epochs, train data loader, and validation data loader\n",
        "results = fit(\n",
        "    model = ...,\n",
        "    epoch = ...,\n",
        "    train_loader = ...,\n",
        "    val_loader = ...\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "iCDJXY5mV23q"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw48D8F2V23q"
      },
      "source": [
        "### Plot the model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMvs9dtHV23r"
      },
      "outputs": [],
      "source": [
        "plot_performance(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjBoorynV23r"
      },
      "source": [
        "### NEXT ROUND: Retrain the model with frozen layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iH50oGbV23s"
      },
      "outputs": [],
      "source": [
        "FROZEN_RESULTS = []\n",
        "for idx, m in enumerate([model_freeze_block1, model_freeze_block12]):\n",
        "    print(\"id: {}\".format(idx))\n",
        "    trainable_model_params = [p for p in m.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(trainable_model_params, lr=LEARNING_RATE)\n",
        "\n",
        "    new_results = fit(model=m, epoch=EPOCH, train_loader=train_loader, val_loader=val_loader)\n",
        "    FROZEN_RESULTS.append(new_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua9XK664V23s"
      },
      "source": [
        "### Examine the difference in both accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhOvNRhnV23t"
      },
      "outputs": [],
      "source": [
        "plot_performance(FROZEN_RESULTS[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpU86rI7V23t"
      },
      "outputs": [],
      "source": [
        "plot_performance(FROZEN_RESULTS[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFJg2iI3V23t"
      },
      "outputs": [],
      "source": [
        "# QUESTIONS\n",
        "# TODO: With the same 5 epochs in training, why Transfer Learning with frozen layers are worse in the final accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIO43pQeV23u"
      },
      "source": [
        "\n",
        "\n",
        "1. **Over-Constraints and Feature Mismatch**: Freezing a substantial number of layers during transfer learning can lead to over-constraints, especially if the target task significantly differs from the pre-training task. The model's ability to adapt to new features may be constrained, resulting in reduced final accuracy. Additionally, if the pre-trained layers capture high-level features that are irrelevant or less relevant to the target task, freezing them may hinder the model's ability to adapt to the nuances of the new task. This can result in suboptimal performance.\n",
        "\n",
        "2. **Loss of Task-Specific Information and Balance**: Freezing too many layers may lead to the loss of task-specific information. The model becomes constrained to using only generic features learned during pre-training, which may not be sufficient for the target task. Achieving the right balance between the number of frozen and trainable layers is essential. If too many layers are frozen, the model may not adapt effectively, while too few frozen layers might lead to overfitting and longer training times.\n",
        "\n",
        "3. **Dataset Size and Hyperparameter Tuning**: The size of the target dataset plays a significant role. If the target dataset is small, fine-tuning with too many frozen layers can lead to poor generalization because the model may rely heavily on pre-trained features. Additionally, the choice of hyperparameters such as learning rate, weight decay, and batch size can impact the training process. Fine-tuning with more frozen layers may require different hyperparameter settings to achieve the best performance.\n",
        "\n",
        "4. **The Role of Early Layers**: In many neural networks, early layers capture basic features like edges and textures. When you freeze these early layers, the model may lose the ability to adapt to the specific patterns and structures of the new task. This can result in a loss of discriminative power and reduced final accuracy.\n",
        "\n",
        "5. **Striking the Right Balance**: The decision of how many layers to freeze during transfer learning is a trade-off. It depends on the nature of the target task, the similarity of the pre-training and target domains, the size of the dataset, and the effectiveness of hyperparameter settings. Experimentation and hyperparameter tuning are often necessary to find the optimal configuration that maximizes the final accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIb644GNV23u"
      },
      "outputs": [],
      "source": [
        "# QUESTIONS\n",
        "# TODO: Why the more layers are frozen, the lower the accuracy of the model in the early (the 1st) epoch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtv1O48sV23u"
      },
      "source": [
        "\n",
        "\n",
        "1. **Inadequate Fine-Tuning in Early Epochs**: When more layers are frozen, especially in the early epochs of model training, the model may struggle to adapt to the new task's specific features and patterns. The task-specific information may not be adequately learned, leading to lower accuracy in the initial epoch.\n",
        "\n",
        "2. **Loss of Task-Specific Adaptation**: Freezing numerous layers restricts the model's ability to adapt to the unique features of the target task. In the early stages, the model needs to rapidly acquire task-specific knowledge, but with many layers frozen, it relies on pre-learned, generic features. This may result in lower accuracy.\n",
        "\n",
        "3. **Over-Constrained Model**: The over-constraint problem arises when a large number of layers are frozen. It limits the model's flexibility to adjust its weights to the new task's requirements. During the early epochs, this constraint can hinder the model's ability to make substantial weight updates, causing lower accuracy.\n",
        "\n",
        "4. **Feature Mismatch and Irrelevance**: Pre-trained layers can contain high-level features that are either unrelated or less relevant to the target task. Freezing these layers can impede the model's capacity to adapt to the unique features and patterns of the new task, potentially causing lower accuracy early in training.\n",
        "\n",
        "5. **Weight Initialization and Suboptimal Configurations**: When many layers are frozen, the model may not receive the necessary weight updates during the early epochs, which can result in suboptimal weight configurations. This suboptimality can contribute to lower accuracy, especially in the initial stages of training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFlZouUDV23v"
      },
      "source": [
        "### Examine the difference in the execution time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9w_0AAkV23v"
      },
      "outputs": [],
      "source": [
        "print(\"When all layers were TRAINABLE: {:.3f}s.\".format(results[\"exc_time\"]))\n",
        "print(\"Only 'denseblock1' was FROZEN: {:.3f}s.\".format(FROZEN_RESULTS[0][\"exc_time\"]))\n",
        "print(\"Only 'denseblock1' and 'denseblock2' wwere FROZEN: {:.3f}s.\".format(FROZEN_RESULTS[1][\"exc_time\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbm_3QaBV23v"
      },
      "outputs": [],
      "source": [
        "# QUESTIONS\n",
        "# TODO: Why the more layers are frozen, the faster the training-validation time?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzkxY5yMV23v"
      },
      "source": [
        "\n",
        "\n",
        "1. **Fewer Parameters to Update**: Freezing more layers during fine-tuning significantly speeds up training-validation times. This is because when layers are frozen, their weights and biases remain constant, reducing the number of parameters that require updates during training. Fewer parameter updates lead to faster training, with less time spent on gradient computations and weight adjustments. This is especially advantageous when working with deep neural networks or limited computational resources.\n",
        "\n",
        "2. **Reduced Computational Load**: When layers are frozen, there is no need to compute gradients and perform backpropagation for those layers. This reduces the computational workload during training, making forward and backward passes through the network quicker. Consequently, the overall training process becomes more time-efficient.\n",
        "\n",
        "3. **Optimized Memory Usage**: Freezing layers also offers memory efficiency. Since the model doesn't need to store activations, gradients, and weight updates for the frozen layers, it requires less memory during training. This is particularly important when working with GPUs with limited memory capacity.\n",
        "\n",
        "4. **Enhanced Training Stability**: Freezing layers can contribute to training stability. By keeping lower-level features fixed, the model is less likely to overfit to the training data. It prevents the model from making drastic changes to the learned representations, leading to quicker convergence. This is especially beneficial when adapting pre-trained models to new tasks, ensuring a more stable training process.\n",
        "\n",
        "5. **Transfer Learning Advantages**: Transfer learning often leverages pre-trained models with numerous frozen layers. These pre-trained models have already learned valuable feature representations from extensive datasets. By keeping these layers frozen, you inherit their feature extraction capabilities, which are adapted to your specific task. This accelerates the training process, as the model starts with knowledge of relevant features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataset Minset\n"
      ],
      "metadata": {
        "id": "jfTajIgZ0nRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Transformations to apply to the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "    transforms.ToTensor()  # Convert to tensor\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Create data loaders for training and testing\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqRLAPpU0xxs",
        "outputId": "bd7b4da5-cf39-4dfb-a5c2-0020357d40c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 81923068.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 110424515.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 42209963.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3247064.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DenseNet\n"
      ],
      "metadata": {
        "id": "oHgM6ldu06fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained DenseNet model\n",
        "densenet = models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the first layer (input layer)\n",
        "densenet.features[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer (output layer)\n",
        "densenet.classifier = nn.Linear(1024, 10)\n"
      ],
      "metadata": {
        "id": "qgHMqPRI1BBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ResNet"
      ],
      "metadata": {
        "id": "wk7Nweqp1HOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained ResNet model\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "# Modify the first layer (input layer)\n",
        "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer (output layer)\n",
        "resnet.fc = nn.Linear(512, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhix5xVy1LhQ",
        "outputId": "a929dc0b-4edf-4a18-c49b-5895e785b76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 161MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "lDH8RkzjvfoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm.autonotebook import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from copy import deepcopy\n",
        "from warnings import filterwarnings as fw\n",
        "\n",
        "# Setting random seeds for reproducibility\n",
        "seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Check PyTorch and Torchvision versions\n",
        "assert torch.__version__ == \"2.0.1+cu117\"\n",
        "assert torchvision.__version__ == \"0.15.2+cu117\"\n",
        "\n",
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "                               transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load pre-trained DenseNet model\n",
        "densenet = torchvision.models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the first layer (input layer)\n",
        "densenet.features[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer (output layer)\n",
        "densenet.classifier = nn.Linear(1024, 10)\n",
        "\n",
        "# Print the modified DenseNet model\n",
        "print(densenet)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(densenet.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 5  # You can adjust this number based on your needs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "densenet.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    densenet.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = densenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "densenet.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = densenet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on the test set: {100 * correct / total}%\")\n"
      ],
      "metadata": {
        "id": "-l1NLIbzwx4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Plot performa model, baik untuk hasil pelatihan maupun validasi."
      ],
      "metadata": {
        "id": "GMkDCqvIxLpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store training and validation loss for plotting\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "densenet.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    densenet.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = densenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation loss\n",
        "    densenet.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = densenet(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    val_loss = val_loss / len(test_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.figure()\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "densenet.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = densenet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on the test set: {100 * correct / total}%\")\n"
      ],
      "metadata": {
        "id": "eUFuiRR2xVb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Sekarang, coba bekukan (freeze, tidak dilatih) beberapa bagian layer: (1) \"denseblock1\", (2)\n",
        "\"denseblock1\" dan \"denseblock2\". Ini akan menjadi dua model terpisah."
      ],
      "metadata": {
        "id": "vYIDasIQxdRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm.autonotebook import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from copy import deepcopy\n",
        "from warnings import filterwarnings as fw\n",
        "\n",
        "# Setting random seeds for reproducibility\n",
        "seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Check PyTorch and Torchvision versions\n",
        "assert torch.__version__ == \"2.0.1+cu117\"\n",
        "assert torchvision.__version__ == \"0.15.2+cu117\"\n",
        "\n",
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "                               transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load pre-trained DenseNet model\n",
        "densenet = torchvision.models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the first layer (input layer)\n",
        "densenet.features[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer (output layer)\n",
        "densenet.classifier = nn.Linear(1024, 10)\n",
        "\n",
        "# Create a copy of the model for the second model\n",
        "densenet2 = deepcopy(densenet)\n",
        "\n",
        "# Freeze layers in \"denseblock1\" of the first model\n",
        "for param in densenet.features.denseblock1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze layers in \"denseblock1\" and \"denseblock2\" of the second model\n",
        "for param in densenet2.features.denseblock1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in densenet2.features.denseblock2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define the loss function and optimizer for both models\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer1 = optim.Adam(densenet.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(densenet2.parameters(), lr=0.001)\n",
        "\n",
        "# Training the first model\n",
        "num_epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "densenet.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    densenet.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} (Model 1)\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer1.zero_grad()\n",
        "        outputs = densenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} (Model 1), Training Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Training the second model\n",
        "densenet2.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    densenet2.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} (Model 2)\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer2.zero_grad()\n",
        "        outputs = densenet2(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} (Model 2), Training Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluate the first model on the test set\n",
        "densenet.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = densenet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of Model 1 on the test set: {100 * correct / total}%\")\n",
        "\n",
        "# Evaluate the second model on the test set\n",
        "densenet2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_d\n"
      ],
      "metadata": {
        "id": "9qNFKfWPyesL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Latih kembali setiap model, visualisasikan performanya, dan periksa perbedaannya"
      ],
      "metadata": {
        "id": "yIcBW674y_ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm.autonotebook import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from copy import deepcopy\n",
        "from warnings import filterwarnings as fw\n",
        "\n",
        "# Setting random seeds for reproducibility\n",
        "seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Check PyTorch and Torchvision versions\n",
        "assert torch.__version__ == \"2.0.1+cu117\"\n",
        "assert torchvision.__version__ == \"0.15.2+cu117\"\n",
        "\n",
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "                               transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load pre-trained DenseNet model\n",
        "densenet = torchvision.models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the first layer (input layer)\n",
        "densenet.features[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer (output layer)\n",
        "densenet.classifier = nn.Linear(1024, 10)\n",
        "\n",
        "# Create a copy of the model for the second model\n",
        "densenet2 = deepcopy(densenet)\n",
        "\n",
        "# Freeze layers in \"denseblock1\" of the first model\n",
        "for param in densenet.features.denseblock1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze layers in \"denseblock1\" and \"denseblock2\" of the second model\n",
        "for param in densenet2.features.denseblock1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in densenet2.features.denseblock2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define the loss function and optimizer for both models\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer1 = optim.Adam(densenet.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(densenet2.parameters(), lr=0.001)\n",
        "\n",
        "# Training the first model\n",
        "num_epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "densenet.to(device)\n",
        "\n",
        "train_losses1 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    densenet.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} (https://colab.research.google.com/drive/19HD_nfQW2Zo54hjrUlx_UHfFHAQ9J3x1#scrollTo=qgHMqPRI1BBq&line=2&uniqifier=1)\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer1.zero_grad()\n",
        "        outputs = densenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_losses1.append(train_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} (https://colab.research.google.com/drive/19HD_nfQW2Zo54hjrUlx_UHfFHAQ9J3x1#scrollTo=qgHMqPRI1BBq&line=2&uniqifier=1), Training Loss: {train_loss}\")\n",
        "\n",
        "# Training the second model\n",
        "densenet2.to(device)\n",
        "\n",
        "train_losses2 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    densenet2.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} (Model 2)\"):\n",
        "        images, labels = images to(device), labels.to(device)\n",
        "        optimizer2.zero_grad()\n",
        "        outputs = densenet2(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_losses2.append(train_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} (https://colab.research.google.com/drive/19HD_nfQW2Zo54hjrUlx_UHfFHAQ9J3x1#scrollTo=qgHMqPRI1BBq&line=2&uniqifier=1), Training Loss: {train_loss}\")\n",
        "\n",
        "# Evaluate the first model on the test set\n",
        "densenet.eval()\n",
        "correct1 = 0\n",
        "total1 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = densenet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total1 += labels.size(0)\n",
        "        correct1 += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy1 = 100 * correct1 / total1\n",
        "\n",
        "# Evaluate the second model on the test set\n",
        "densenet2.eval()\n",
        "correct2 = 0\n",
        "total2 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = densenet2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy2 = 100 * correct2 / total2\n",
        "\n",
        "# Visualize the training loss for both models\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses1, label='Training Loss (https://colab.research.google.com/drive/19HD_nfQW2Zo54hjrUlx_UHfFHAQ9J3x1#scrollTo=qgHMqPRI1BBq&line=2&uniqifier=1)')\n",
        "plt.plot(range(1, num_epochs + 1), train_losses2, label='Training Loss (Model 2)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss for Model 1 and Model 2')\n",
        "plt.show()\n",
        "\n",
        "# Print and compare the accuracy of both models\n",
        "print(f\"Accuracy of Model 1 on the test set: {accuracy1}%\")\n",
        "print(f\"Accuracy of Model 2 on the test set: {accuracy2}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "3UcT6VwezCW0",
        "outputId": "1c107b3b-0d39-47d7-fca4-9779302cd643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-9fbd3cd25f61>\"\u001b[0;36m, line \u001b[0;32m92\u001b[0m\n\u001b[0;31m    images, labels = images to(device), labels.to(device)\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize the training loss for Model 1 (DenseNet)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses1, label='Training Loss (DenseNet)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss for DenseNet (Model 1)')\n",
        "plt.show()\n",
        "\n",
        "# Visualize the training loss for Model 2 (ResNet)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses2, label='Training Loss (ResNet)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss for ResNet (Model 2)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "7ng-lBEx1dc4",
        "outputId": "9ec6a6b0-95f3-45da-8ce5-facd181fc058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-be39a32f3c29>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Visualize the training loss for Model 1 (DenseNet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss (DenseNet)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses1' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Anda mereplikasi semua langkah di atas dengan model yang berbeda,\n",
        "misalnya ResNet"
      ],
      "metadata": {
        "id": "7mGMhSOazl3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm.autonotebook import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from copy import deepcopy\n",
        "from warnings import filterwarnings as fw\n",
        "\n",
        "# Setting random seeds for reproducibility\n",
        "seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Check PyTorch and Torchvision versions\n",
        "assert torch.__version__ == \"2.0.1+cu117\"\n",
        "assert torchvision.__version__ == \"0.15.2+cu117\"\n",
        "\n",
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "                               transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load pre-trained DenseNet model\n",
        "densenet = torchvision.models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the first layer (input layer)\n",
        "densenet.features[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer (output layer)\n",
        "densenet.classifier = nn.Linear(1024, 10)\n",
        "\n",
        "# Create a copy of the model for the second model\n",
        "densenet2 = deepcopy(densenet)\n",
        "\n",
        "# Freeze layers in \"denseblock1\" of the first model\n",
        "for param in densenet.features.denseblock1.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze layers in \"denseblock1\" and \"denseblock2\" of the second model\n",
        "for param in densenet2.features.denseblock1.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in densenet2.features.denseblock2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define the loss function and optimizer for both models\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer1 = optim.Adam(densenet.parameters(), lr=0.001)\n",
        "optimizer2 = optim.Adam(densenet2.parameters(), lr=0.001)\n",
        "\n",
        "# Training the first model\n",
        "num_epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "densenet.to(device)\n",
        "\n",
        "train_losses1 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    densenet.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} (Model 1)\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer1.zero_grad()\n",
        "        outputs = densenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_losses1.append(train_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} (Model 1), Training Loss: {train_loss}\")\n",
        "\n",
        "# Training the second model\n",
        "densenet2.to(device)\n",
        "\n",
        "train_losses2 = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    densenet2.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1} (Model 2)\"):\n",
        "        images, labels = images to(device), labels.to(device)\n",
        "        optimizer2.zero_grad()\n",
        "        outputs = densenet2(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_losses2.append(train_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} (Model 2), Training Loss: {train_loss}\")\n",
        "\n",
        "# Evaluate the first model on the test set\n",
        "densenet.eval()\n",
        "correct1 = 0\n",
        "total1 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = densenet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total1 += labels.size(0)\n",
        "        correct1 += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy1 = 100 * correct1 / total1\n",
        "\n",
        "# Evaluate the second model on the test set\n",
        "densenet2.eval()\n",
        "correct2 = 0\n",
        "total2 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = densenet2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy2 = 100 * correct2 / total2\n",
        "\n",
        "# Visualize the training loss for both models\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses1, label='Training Loss (Model 1)')\n",
        "plt.plot(range(1, num_epochs + 1), train_losses2, label='Training Loss (Model 2)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Loss for Model 1 and Model 2')\n",
        "plt.show()\n",
        "\n",
        "# Print and compare the accuracy of both models\n",
        "print(f\"Accuracy of Model 1 on the test set: {accuracy1}%\")\n",
        "print(f\"Accuracy of Model 2 on the test set: {accuracy2}%\")\n"
      ],
      "metadata": {
        "id": "JxivQQlezmLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd8Td1YsV23w"
      },
      "source": [
        "### Scoring\n",
        "Total `#TODO` = 12\n",
        "<br>Checklist:\n",
        "\n",
        "- [ ] Change the DenseNet input layer stack by calling create_conv2d()\n",
        "- [ ] Change the DenseNet output layer with 10 classes\n",
        "- [ ] Change the ViT input layer stack by calling create_conv2d()\n",
        "- [ ] Change the ViT output layer with 10 classes\n",
        "- [ ] Define the batch size\n",
        "- [ ] Define the learning rate\n",
        "- [ ] Define the loss function (for multi-classification)\n",
        "- [ ] Pass the string \"resnet\" for ResNet18, \"densenet\" for DenseNet121, and \"vit\" for Vision Transformer\n",
        "- [ ] Specify variables for your model, number of epochs, train data loader, and validation data loader\n",
        "- [ ] QUESTION: With the same 5 epochs in training, why Transfer Learning with frozen layers are worse in the final accuracy?\n",
        "- [ ] QUESTION: Why the more layers are frozen, the lower the accuracy of the model in the early (the 1st) epoch?\n",
        "- [ ] QUESTION: Why the more layers are frozen, the faster the training-validation time?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu54eCVrV23w"
      },
      "source": [
        "### Additional readings\n",
        "* ResNet: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\n",
        "* DenseNet: https://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf\n",
        "* Vision Transformer (ViT): https://arxiv.org/pdf/2010.11929.pdf\n",
        "* MNIST Classification w/ PyTorch (Beginner): https://www.kaggle.com/code/amsharma7/mnist-pytorch-for-beginners-detailed-desc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lozabSPV23w"
      },
      "source": [
        "### Copyright © 2023 Startup Campus, Indonesia\n",
        "* You may **NOT** use this file except there is written permission from PT. Kampus Merdeka Belajar (Startup Campus).\n",
        "* Please address your questions to mentors."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}